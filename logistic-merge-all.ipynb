{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzhan273/.conda/envs/nlp/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load word emb takes time: 100.60068774223328\n",
      "WARNING: You have a CUDA device\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from textrank import TextRank4Keyword\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# You will have to download the set of stop words the first time\n",
    "\n",
    "# from utils import text_to_wordlist_no_stop\n",
    "# from utils import text_to_wordlist_no_stop\n",
    "from utils import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tprint(\"WARNING: You have a CUDA device\")\n",
    "\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Parameters\n",
    "\n",
    "# all 3639 samples\n",
    "# train 2911 samples\n",
    "# text_piece = 100  # if change, also modify in chunks()\n",
    "batch_size = 40\n",
    "n_classes = 4\n",
    "\n",
    "epochs = 6\n",
    "log_interval = 1000\n",
    "lr = 0.01\n",
    "clip = 0.25\n",
    "\n",
    "\n",
    "'''Model Param'''\n",
    "output_size = n_classes\n",
    "embedding_dim = glove_dim\n",
    "hidden_dim = glove_dim\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get text list\n",
      "| epoch   0 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.08 |\n",
      "| epoch   0 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.08 |\n",
      "0.195064071353025\n",
      "| epoch   1 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.06 |\n",
      "| epoch   1 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.06 |\n",
      "0.14737033998723909\n",
      "| epoch   2 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.06 |\n",
      "| epoch   2 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.06 |\n",
      "0.15790655423008365\n",
      "| epoch   3 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.05 |\n",
      "| epoch   3 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.04 |\n",
      "0.301935233950663\n",
      "| epoch   4 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.04 |\n",
      "| epoch   4 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.04 |\n",
      "0.2652354394115312\n",
      "| epoch   5 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.05 |\n",
      "| epoch   5 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.05 |\n",
      "0.3122048378172629\n",
      "| epoch   6 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.05 | loss  0.06 |\n",
      "| epoch   6 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.05 | loss  0.06 |\n",
      "0.17673687137246724\n",
      "| epoch   7 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.08 |\n",
      "| epoch   7 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.09 |\n",
      "0.2350928641251222\n",
      "| epoch   8 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.11 |\n",
      "| epoch   8 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.10 |\n",
      "0.12547677369768484\n",
      "| epoch   9 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.11 |\n",
      "| epoch   9 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.08 |\n",
      "0.3168470679411005\n",
      "| epoch  10 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.10 |\n",
      "| epoch  10 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.08 |\n",
      "0.2326770576975663\n",
      "| epoch  11 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.11 |\n",
      "| epoch  11 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.11 |\n",
      "0.13718400674922415\n",
      "| epoch  12 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.13 |\n",
      "| epoch  12 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.14 |\n",
      "0.2259066753294351\n",
      "| epoch  13 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.13 |\n",
      "| epoch  13 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.11 |\n",
      "0.34843922694669616\n",
      "| epoch  14 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.07 |\n",
      "| epoch  14 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.08 |\n",
      "0.3228255126339941\n",
      "| epoch  15 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.13 |\n",
      "| epoch  15 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.14 |\n",
      "0.13770940721649483\n",
      "| epoch  16 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.16 |\n",
      "| epoch  16 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.16 |\n",
      "0.1828643216080402\n",
      "| epoch  17 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.23 |\n",
      "| epoch  17 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.12 |\n",
      "0.32373218711737406\n",
      "| epoch  18 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.11 |\n",
      "| epoch  18 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.10 |\n",
      "0.17590145211290587\n",
      "| epoch  19 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.15 |\n",
      "| epoch  19 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.15 |\n",
      "0.18943832528151125\n",
      "| epoch  20 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.06 | loss  0.17 |\n",
      "| epoch  20 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.07 | loss  0.19 |\n",
      "0.2690767973856209\n",
      "| epoch  21 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.06 | loss  0.27 |\n",
      "| epoch  21 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.06 | loss  0.18 |\n",
      "0.27838547613610415\n",
      "| epoch  22 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.06 | loss  0.23 |\n",
      "| epoch  22 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.06 | loss  0.21 |\n",
      "0.3378536922015183\n",
      "| epoch  23 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.07 | loss  0.17 |\n",
      "| epoch  23 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.07 | loss  0.21 |\n",
      "0.2772808435361214\n",
      "| epoch  24 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.23 |\n",
      "| epoch  24 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.25 |\n",
      "0.1866725291595758\n",
      "| epoch  25 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.22 |\n",
      "| epoch  25 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.29 |\n",
      "0.2343255603711878\n",
      "| epoch  26 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.22 |\n",
      "| epoch  26 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.25 |\n",
      "0.2707337342593438\n",
      "| epoch  27 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.21 |\n",
      "| epoch  27 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.18 |\n",
      "0.28371245000143563\n",
      "| epoch  28 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.26 |\n",
      "| epoch  28 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.29 |\n",
      "0.20383578523253654\n",
      "| epoch  29 |  1000/ 2900 batches | lr 0.01 | ms/batch  0.04 | loss  0.33 |\n",
      "| epoch  29 |  2000/ 2900 batches | lr 0.01 | ms/batch  0.03 | loss  0.33 |\n",
      "0.19821267260291653\n",
      "severity/text_frightening_severity.csv best f1 0.34843922694669616\n",
      "get text list\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c711bf420db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# whole dataset text_list = num of article * one big word strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get text list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtext_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_wordlist_no_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# remove punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtext_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_wordlist_no_punc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/home/yzhan273/Research/MPAA/Severity_Class_Pred/utils.py\u001b[0m in \u001b[0;36mtext_to_wordlist_no_stop\u001b[0;34m(raw_text)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# return raw_text long string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# stop_words = stopwords.words('english')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mtokenized_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mwordlist_no_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist_no_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/home/yzhan273/Research/MPAA/Severity_Class_Pred/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# return raw_text long string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# stop_words = stopwords.words('english')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mtokenized_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mwordlist_no_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist_no_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_files = ['severity/text_frightening_severity.csv',\n",
    "             'severity/text_alcohol_severity.csv',\n",
    "             'severity/text_nudity_severity.csv',\n",
    "             'severity/text_violence_severity.csv',\n",
    "             'severity/text_profanity_severity.csv']\n",
    "\n",
    "# all_files = ['severity/text_alcohol_severity.csv',\n",
    "#              'severity/text_nudity_severity.csv',\n",
    "#              'severity/text_violence_severity.csv',\n",
    "#              'severity/text_profanity_severity.csv']\n",
    "\n",
    "f_baseline_filename = \"all_file_logistic_baseline_record.txt\"\n",
    "\n",
    "with open(f_baseline_filename, \"a\") as f_baseline:\n",
    "    for script in range(len(all_files)):\n",
    "        f_baseline.write(all_files[script])\n",
    "        f_baseline.write('\\n')\n",
    "        all_data = pd.read_csv(all_files[script], sep='\\t')\n",
    "        data_batch = all_data\n",
    "\n",
    "        label_batch = data_batch['label']\n",
    "        text_batch = data_batch['text']\n",
    "\n",
    "        # reconstruct each text into long list of words\n",
    "        # whole dataset text_list = num of article * one big word strings\n",
    "        print('get text list')\n",
    "        text_list = list(map(text_to_wordlist_no_stop, text_batch))\n",
    "        # remove punctuation\n",
    "        text_list = list(map(text_to_wordlist_no_punc, text_list))\n",
    "        # do it again\n",
    "        text_list = list(map(text_to_wordlist_no_stop, text_list))\n",
    "\n",
    "        ##### #key word method\n",
    "        # cur_time = time.time()\n",
    "        # text_list = get_key_words_onlist(text_list)\n",
    "        # print('get key words list takes time:', time.time() - cur_time)\n",
    "        # print(text_list)\n",
    "\n",
    "        ### word vector glove\n",
    "        text_doc_emb_list = np.array(list(map(doc_emb, text_list)))\n",
    "\n",
    "        X = text_doc_emb_list\n",
    "\n",
    "        # label_batch_digits = label_to_one_hot(label_batch, n_classes)\n",
    "        label_batch_digits = label_to_digits(label_batch)\n",
    "\n",
    "        y = label_batch_digits\n",
    "\n",
    "        training_size = int(0.8 * 3600) # len(X)\n",
    "        validation_size = int(0.1 * 3600)\n",
    "        test_size = int(0.1 * 3600)\n",
    "\n",
    "        X_train = X[0:training_size]\n",
    "        X_dev = X[training_size:(training_size + validation_size)]\n",
    "        X_test = X[(training_size + validation_size):(training_size + validation_size + test_size)]\n",
    "\n",
    "        y_train = y[0:training_size]\n",
    "        y_dev = y[training_size:(training_size + validation_size)]\n",
    "        y_test = y[(training_size + validation_size):(training_size + validation_size + test_size)] \n",
    "\n",
    "        # NN try\n",
    "        X_train,X_dev,X_test  = \\\n",
    "        X_train.astype(np.float32),X_dev.astype(np.float32),X_test.astype(np.float32)\n",
    "      \n",
    "\n",
    "        from sklearn.metrics import f1_score    \n",
    "        \n",
    "        class LogisticRegression(torch.nn.Module):\n",
    "            def __init__(self, input_dim, output_dim):\n",
    "                super(LogisticRegression, self).__init__()\n",
    "                self.linear = torch.nn.Linear(input_dim, input_dim)\n",
    "                nn.init.uniform_(self.linear.weight)\n",
    "#                 self.dropout = nn.Dropout(p=0.25)\n",
    "                self.linear2 = torch.nn.Linear(input_dim, output_dim)\n",
    "                nn.init.uniform_(self.linear2.weight)\n",
    "\n",
    "\n",
    "            def forward(self, x):\n",
    "                outputs = self.linear(x)\n",
    "#                 outputs = self.dropout(outputs)\n",
    "                outputs = self.linear2(outputs)\n",
    "#                 y_pred = F.sigmoid(outputs)\n",
    "\n",
    "                return outputs\n",
    "\n",
    "        LR_model = LogisticRegression(embedding_dim, n_classes)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "\n",
    "#         optimizer = torch.optim.SGD(LR_model.parameters(), lr=0.001)\n",
    "        optimizer = torch.optim.Adam(LR_model.parameters())\n",
    "\n",
    "        epochs = 30\n",
    "        \n",
    "        best_f1 = []\n",
    "        for epoch in range(epochs):    \n",
    "            LR_model.train()\n",
    "            total_loss = 0.\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for i in range(0,X_train.shape[0],batch_size):\n",
    "                \n",
    "                output = LR_model(torch.Tensor(X_train[i:i + batch_size]))\n",
    "\n",
    "                loss = criterion(output, y_train[i:i + batch_size])\n",
    "    \n",
    "                # loss.backward()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#                 print(LR_model.linear.weight)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if i % log_interval == 0 and i > 0:\n",
    "                    cur_loss = total_loss / log_interval\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                          'loss {:5.2f} |'.format(\n",
    "                        epoch, i, X[0:partition].shape[0], lr,\n",
    "                        elapsed * 1000 / log_interval, cur_loss))\n",
    "                    total_loss = 0\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "            validation_output = LR_model(torch.Tensor(X_dev))\n",
    "            \n",
    "            \n",
    "#             clas_report = classification_report(y_dev.numpy(), np.argmax(validation_output.detach().numpy(), axis=1))\n",
    "#             print(clas_report)\n",
    "            f1score = f1_score(y_dev.numpy(), np.argmax(validation_output.detach().numpy(), axis=1), average='macro')\n",
    "            print(f1score)\n",
    "            best_f1.append(f1score)\n",
    "        print(all_files[script], 'best f1',max(best_f1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
